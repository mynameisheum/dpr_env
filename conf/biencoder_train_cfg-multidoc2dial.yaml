
# configuration groups
defaults:
  - encoder: hf_bert
  - train: biencoder_default-multidoc2dial
  - datasets: encoder_train_default



#train_datasets: /home/tjdgma/tjdgma/multidoc2dial_project/data_renew/mdd_dpr_renew/dpr.multidoc2dial_all.structure.train_renew_both5-15.json
train_datasets: /home/tjdgma/DPR/dpr/checkpoints_multidoc2dial_renew_ANCE_grounding_based_bm25/3-update/3-2update/h3/dpr.multidoc2dial_all.structure.train_renew_ANCE_grounding_based_for_h3_3-2update_60infer.json
dev_datasets:
#output_dir: /home/tjdgma/DPR/dpr/checkpoints_multidoc2dial_renew_ANCE_grounding_based_bm25/1-update
output_dir: /home/tjdgma/DPR/dpr/checkpoints_multidoc2dial_renew_ANCE_grounding_based_bm25/3-update/3-2update/h3

train_sampling_rates:
loss_scale_factors:

# Whether to lower case the input text. Set True for uncased models, False for the cased ones.
do_lower_case: True

val_av_rank_start_epoch: 0
seed: 12345
checkpoint_file_name: dpr_biencoder

# A trained bi-encoder checkpoint file to initialize the model
## change ##
#model_file: /home/tjdgma/DPR/dpr/data/download_data/downloads/checkpoint/retriever/NQ_based/single-adv-hn/nq/bert-base-encoder.cp
model_file: /home/tjdgma/DPR/dpr/checkpoints_multidoc2dial_renew_ANCE_grounding_based_bm25/3-update/3-1update/h3/dpr_biencoder.15
# TODO: move to a conf groupRuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
# local_rank for distributed training on gpus

# TODO: rename to distributed_rank
local_rank: -1
global_loss_buf_sz: 592000
device:
distributed_world_size:
distributed_port:
distributed_init_method:

no_cuda: False
n_gpu: 1
fp16: False

# For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#        "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: O1

# tokens which won't be slit by tokenizer
special_tokens:

## change ##
ignore_checkpoint_offset: False
ignore_checkpoint_optimizer: False
ignore_checkpoint_lr: False

# set to >1 to enable multiple query encoders
multi_q_encoder: False

# Set to True to reduce memory footprint and loose a bit the full train data randomization if you train in DDP mode
local_shards_dataloader: False


# biencoder_default-multidoc2dial

# passage token 256     0000
# num_train_epochs 50   0000
# warmup          400   0000
# lr              2e-5  0000



# epoch save        00000
# init_epoch        00000
# steps_shift +=835 00000

# model file        00000
# output dir        00000
# train dataset     00000

# ignore checkpoint offset  00000
# other neg or hard neg     00000

# Method(init epoch option) : random    000xx

#positive_idx_per_question [0,1,2,3]    xxxx
#sampling = RS sampling                 xxxx

# infer_60                              xxx00